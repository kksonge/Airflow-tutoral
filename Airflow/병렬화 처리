#### 단순 병렬화 ####

rfm_clustering_tasks = []

for rfm_module in py_execute_biz_modules['RFM']:
  rfm_clustering_task = EmrAddStepsOperator(
      task_id='rfm_clustering_{}_emr'.format(rfm_module),
      job_flow_id=create_job_flow_id,
      aws_conn_id='aws_default',
      queue='main',
      steps=get_pyspark_execute_rfm(rfm_module),
      dag=dag
    )
   rfm_clustering_tasks.append(rfm_clustering_task)
   
execute_biz_starter >> rfm_clustering_tasks >> execute_biz_end

#### 내장 모듈인 chain 사용한 방법 ####

from airflow.utils.helpers import chain

rfm_clustering_tasks = []

for rfm_module in range(3):
    rfm_clustering_tasks.append(EmrAddStepsOperator(
        task_id='rfm_clustering_{}_emr'.format(py_execute_biz_modules['RFM'][rmf_module]),
        job_flow_id=created_job_flow_id,
        aws_conn_id='aws_default',
        queue='main',
        steps=get_pyspark_execute_rfm(py_execute_biz_modules['RFM'][rfm_module]),
        dag=dag
      )
    )
    
chain(execute_biz_starter, rfm_clustering_tasks, execute_biz_end)
