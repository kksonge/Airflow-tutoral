#### for문 사용하여 Dynamic task generate ####

rfm_clustering_tasks = []

for rfm_module in range(3):
    rfm_clustering_tasks.append(EmrAddStepsOperator(
        task_id='rfm_clustering_{}_emr'.format(py_execute_biz_modules['RFM'][rfm_module]),
        job_flow_id=created_job_flow_id,
        aws_conn_id='aws_default',
        queue='main',
        steps=get_pyspark_execute_rfm(py_execute_biz_modules['RFM'][rfm_module]),
        dag=dag
       )
    )
    
    if rfm_module == 0:
        execute_biz_starter >> rfm_clustering_tasks[rfm_module]
    else:
        rfm_clustering_tasks[rfm_module-1] >> rfm_clustering_tasks[rfm_module]
     
rfm_clustering_tasks[-1] >> execute_biz_end


#### 각 task를 갱신하며 추가 ####

previous_task = None

for rfm_module in range(3):
    rfm_clustering_task = EmrAddStepsOperator(
        task_id='rfm_clustering_{}_emr'.format(py_execute_biz_modules['RFM'][rfm_module],
        job_flow_id=created_job_flow_id,
        aws_conn_id='aws_default',
        queue='main',
        steps=get_pyspark_execute_rfm(py_execute_biz_modules['RFM'][rfm_module]),
        dag=dag
    )
    
    if previous_task:
        previoust_task >> rfm_clustering_task
    else:
        execute_biz_starter >> rfm_clustering_task
        
    previous_task = rfm_clustering_task

rfm_clustering_task >> execute_biz_end
    
